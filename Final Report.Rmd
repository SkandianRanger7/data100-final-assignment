---
title: "Final Report"
output: pdf_document
date: "2025-12-01"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggrepel)
library(xgboost)
library(recipes)
library(rsample)
library(tidymodels)
library(tictoc)
library(doParallel)
library(yardstick)
library(ranger)
```

```{r tidying}
aaup_url = "https://lib.stat.cmu.edu/datasets/colleges/aaup.data"
usnews_url = "https://lib.stat.cmu.edu/datasets/colleges/usnews.data"

aaup_cols = c("fed_id", "college_name", "state", "type", "avg_sal_full", "avg_sal_associate", "avg_sal_assistant", "avg_sal_all", "avg_comp_full", "avg_comp_associate", "avg_comp_assistant", "avg_comp_all", "num_full", "num_associate", "num_assistant", "num_instructor", "num_all")

usnews_cols = c("fed_id","college_name","state","public_private","avg_sat_math","avg_sat_verbal","avg_sat_combined","avg_act","q1_sat_math","q3_sat_math","q1_sat_verbal","q3_sat_verbal","q1_act","q3_act","num_apps_received","num_apps_accepted","num_new_enrolled","pct_top10_hs","pct_top25_hs","num_ft_undergrads","num_pt_undergrads","tuition_instate","tuition_outstate","room_board_cost","room_cost","board_cost","fees_additional","books_estimated","personal_spending_est","pct_faculty_phd","pct_faculty_terminal","student_faculty_ratio","pct_alumni_donate","instr_exp_per_student","grad_rate")

calc_iqr = function(q1, q3) q3-q1

aaup = read.csv(aaup_url, header = FALSE, col.names = aaup_cols, na.strings = "*") |>
  mutate(across(starts_with(c("avg", "num")), ~ .x * 100))

usnews = read.csv(usnews_url, header = FALSE, col.names = usnews_cols, na.strings = "*") |>
  mutate(public_private = factor(public_private, levels = c(1, 2), labels = c("public", "private"))) |>
  mutate(sat_math_iqr = calc_iqr(q1_sat_math, q3_sat_math), sat_verbal_iqr = calc_iqr(q1_sat_verbal, q3_sat_verbal))

pub_vs_priv = usnews |>
  group_by(public_private) |>
  summarize(avg_tuition = mean(tuition_instate, na.rm = TRUE), avg_grad_rate = mean(grad_rate, na.rm = TRUE), avg_sat = mean(avg_sat_combined, na.rm = TRUE))

sat_long <- usnews |>
  pivot_longer(
    cols = matches("^(avg|q1|q3)_(sat|act)"),
    names_to = c("stat", "test", "section"),
    names_pattern = "^(avg|q1|q3)_(sat|act)_?(math|verbal)?$",
    values_to = "score"
  ) |>
  select(fed_id, college_name, state, stat, test, section, score)

```

```{r plots}
usnews |>
  select(public_private, tuition_instate, tuition_outstate) |>
  pivot_longer(cols = c(tuition_instate, tuition_outstate), names_to = "instate_outstate", values_to = "tuition") |>
  ggplot() +
    geom_violin(aes(x = public_private, y = tuition, fill = instate_outstate, colour = instate_outstate))

usnews |>
  mutate(state = fct_lump(state, 4)) |>
  ggplot() +
  geom_boxplot(aes(x = reorder(state, grad_rate, FUN=median), y = grad_rate, fill = state)) +
  facet_wrap(~public_private)

usnews_scatter <- usnews |> 
  group_by(state) |>
  summarize(universities = n(),
            percent_private = sum(public_private == "private") / n())

max_uni_state <- usnews_scatter |>
  filter(universities == max(universities))

ggplot(usnews_scatter) +
  geom_point(aes(x = universities, y = percent_private)) +
  geom_label_repel(data = max_uni_state, aes(x = universities, y = percent_private, label = state))
```

```{r modelling}
uni_split <- initial_split(usnews, prop = 0.6, strata = public_private)
uni_train <- training(uni_split)
uni_test <- testing(uni_split)

xgb_tune_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
) |>
  set_engine("xgboost") |>
  set_mode("classification")

xgb_params <- parameters(
  trees(range = c(500, 2000)),
  tree_depth(range = c(3, 10)),
  learn_rate(range = c(-3, -1), trans = log10_trans()) # Log-uniform search
)

rf_tune_spec <- rand_forest(
  trees = 1000,
  mtry = tune(),
  min_n = tune()
) |>
  set_engine("ranger") |> 
  set_mode("classification")

rf_params <- parameters(
  mtry(range = c(5, 30)), 
  min_n(range = c(5, 40))
)

uni_folds <- vfold_cv(uni_train, v = 2, strata = public_private)

cl <- makePSOCKcluster(detectCores() - 4)
registerDoParallel(cl)
print("Parallel processing enabled")

full_recipe <- recipe(public_private ~ ., data = uni_train) |>
  update_role(
    c(fed_id, college_name), 
    new_role = "ID") |>
  step_novel(state) |>
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  step_impute_median(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors())

costless_recipe <- recipe(public_private ~ ., data = uni_train) |>
  update_role(
    c(fed_id, college_name, tuition_instate, tuition_outstate, room_board_cost, room_cost, board_cost, fees_additional, books_estimated, personal_spending_est, instr_exp_per_student), 
    new_role = "ID") |>
  step_novel(state) |>
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  step_impute_median(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors())

apps_recipe <- recipe(public_private ~ num_apps_received + num_new_enrolled + pct_top10_hs, data = uni_train) |>
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  step_impute_median(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors())

workflows <- workflow_set(preproc = list(full = full_recipe, costless = costless_recipe, apps = apps_recipe), 
                             models = list(xgb = xgb_tune_spec, rf = rf_tune_spec))

# Start timer
tic()

bayes_results = list()


map(workflows$wflow_id, function(id) {
  
  current_workflow <- workflows |>
        extract_workflow(id = id)
  
  if (str_detect(id, "rf")) {
    current_params <- rf_params
  } else {
    current_params <- xgb_params
  }
  
  # Run the Bayesian Optimization
  bayes_results[[id]] <<- tune_bayes(
    current_workflow,
    resamples = uni_folds,
    iter = 5,                # Number of iterations (models to fit)
    param_info = current_params,  # Use the defined parameter ranges
    metrics = metric_set(roc_auc, accuracy), # Optimize for AUC
    control = control_bayes(
      no_improve = 10,        # Stop if no improvement after 10 iterations
      save_pred = TRUE,
      verbose = TRUE
  )
)
})

# Stop parallel processing
stopCluster(cl)

# End timer
toc()

#show_best(xgb_bayes_result, metric = "roc_auc")
final_fits = list()
best_params = list()
for (mod_name in names(bayes_results)){
  
  current_best_params <- select_best(bayes_results[[mod_name]], metric = "roc_auc")
  
  best_params[[mod_name]] = current_best_params
  
  final_workflow <- workflows |>
    extract_workflow(id = mod_name) |>
    finalize_workflow(current_best_params)
  
  final_fits[[mod_name]] <- final_workflow |>
    fit(data = uni_train)
}


```


```{r}
# 1. Iterate over the list of final fits and calculate metrics for each
final_results <- map_dfr(
  final_fits, 
  .f = function(model_fit) {
    
    test_predictions <- predict(model_fit, new_data = uni_test, type = "prob") |>
     bind_cols(uni_test |> select(public_private))

    class_predictions <- predict(model_fit, new_data = uni_test, type = "class")

    test_predictions <- test_predictions |>
      bind_cols(class_predictions)
      
    # B. Calculate metrics using the predictions
    # Note: We use .pred_public here, but ensure this is the correct 'event' level
    roc_auc_result <- test_predictions |>
      roc_auc(
        truth = public_private,
        .pred_public
      )
    
    # C. Also calculate accuracy for a second metric
    accuracy_result <- test_predictions |>
      accuracy(
        truth = public_private,
        estimate = .pred_class # Assumes you also generated class predictions
      )
    
    # Combine all metrics into a single row tibble
    bind_rows(roc_auc_result, accuracy_result)
  },
  .id = "model_name" # Create a new column identifying which model the results belong to
)

final_results <- final_results |> pivot_wider(names_from = .metric, values_from = .estimate)

## 2. Print the final tidy table of results
print(final_results)
```